{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddda11a7",
   "metadata": {},
   "source": [
    "# Forest Cover Type — EDA & Preprocessing\n",
    "**Filename:** `Forest_Cover_Type_MT2025065.ipynb` (created programmatically)\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA) and preprocessing on the Forest Cover dataset found in the provided training files. The goal is to prepare clean, reproducible preprocessing steps so training models (including neural networks) is straightforward.\n",
    "\n",
    "**What this notebook contains (high level):**\n",
    "- Data loading and initial inspection\n",
    "- Missing values & dtype checks\n",
    "- Target distribution and class balance\n",
    "- Numeric summaries, feature correlations (sampled)\n",
    "- Suggested preprocessing pipelines (imputation, scaling, encoding)\n",
    "- Example: create training/validation splits and save preprocessed arrays for model training\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fce827",
   "metadata": {},
   "source": [
    "## 1) Dataset loading\n",
    "We loaded the file `covtype.csv` from the provided data. The raw dataframe shape is **(581012, 55)**.\n",
    "\n",
    "**Guessed target column:** `Cover_Type`\n",
    "\n",
    "> If this guess is incorrect, edit the cell where `target_col` is set to the correct column name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2445e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset (head, dtypes, missing values, basic stats)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "csv_path = r\"/mnt/data/archive/covtype.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.shape\n",
    "# show head\n",
    "print(df.head())\n",
    "# dtypes\n",
    "print('\\nDtypes:')\n",
    "print(df.dtypes)\n",
    "# missing\n",
    "print('\\nMissing counts:')\n",
    "print(df.isnull().sum()[df.isnull().sum()>0])\n",
    "# basic stats\n",
    "print('\\nDescribe:')\n",
    "print(df.describe(include='all').T.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2470e25",
   "metadata": {},
   "source": [
    "## 2) EDA highlights & plots\n",
    "We will:\n",
    "- Inspect target distribution\n",
    "- Visualize distributions of key numeric variables (histograms)\n",
    "- Check correlations for numeric features (sampled to keep compute reasonable)\n",
    "\n",
    "Notes about domain: The original UCI Cover Type dataset uses many binary indicator columns for soil and wilderness areas; those are already encoded as 0/1 in many versions. If your file uses a different encoding, convert categorical indicators accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36549d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: target distribution, histograms, correlations (sampled)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "# guess target as before\n",
    "target_col = None\n",
    "for c in df.columns:\n",
    "    if 'cover' in c.lower() and 'type' in c.lower():\n",
    "        target_col = c\n",
    "        break\n",
    "print('Target column guess:', target_col)\n",
    "if target_col is not None:\n",
    "    print('\\nClass distribution:')\n",
    "    print(df[target_col].value_counts().sort_index())\n",
    "\n",
    "# Histograms for first 8 numeric columns\n",
    "cols_to_plot = numeric_cols[:8]\n",
    "for c in cols_to_plot:\n",
    "    plt.figure(figsize=(6,2.5))\n",
    "    plt.hist(df[c].dropna(), bins=40)\n",
    "    plt.title(c)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation heatmap (sample up to 5000 rows)\n",
    "sample = df[numeric_cols].sample(n=min(5000, len(df)), random_state=42)\n",
    "corr = sample.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(corr, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Correlation matrix (numeric features, sampled)')\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr.index)), corr.index)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40bc29",
   "metadata": {},
   "source": [
    "## 3) Preprocessing strategy (templates)\n",
    "We provide two common preprocessing pipelines — **Pipeline A (Standard scaling)** and **Pipeline B (MinMax scaling)** — and show how to apply them.\n",
    "\n",
    "Justifications:\n",
    "- **StandardScaler** is appropriate when features are roughly Gaussian or when using models that assume centered inputs (e.g., linear models, neural networks).\n",
    "- **MinMaxScaler** is useful when preserving original distribution bounds matters (e.g., for some distance-based models or when features are on different scales but bounded).\n",
    "\n",
    "We also provide:\n",
    "- Imputer (median) for numeric features if any missing values exist.\n",
    "- Pass-through for binary indicator columns (soil/wilderness) which don't need scaling in many cases.\n",
    "- Option to use `ColumnTransformer` and `Pipeline` so you can plug into scikit-learn models directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines: create ColumnTransformer examples and show transformed shape\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Identify numeric and binary (0/1) columns heuristically\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "# If target exists, remove it\n",
    "if target_col in numeric_cols:\n",
    "    numeric_cols.remove(target_col)\n",
    "# Heuristic: binary cols are those with unique values subset of {0,1}\n",
    "binary_cols = [c for c in numeric_cols if set(df[c].dropna().unique()).issubset({0,1})]\n",
    "other_numeric = [c for c in numeric_cols if c not in binary_cols]\n",
    "\n",
    "print('Numeric columns:', len(other_numeric), 'Binary columns (likely indicators):', len(binary_cols))\n",
    "\n",
    "num_pipeline_std = Pipeline([('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "num_pipeline_mm = Pipeline([('imputer', SimpleImputer(strategy='median')),('scaler', MinMaxScaler())])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# ColumnTransformer that scales numeric features but leaves binary indicator columns as-is\n",
    "ct_std = ColumnTransformer([\n",
    "    ('num', num_pipeline_std, other_numeric),\n",
    "    ('bin', 'passthrough', binary_cols)\n",
    "])\n",
    "ct_mm = ColumnTransformer([\n",
    "    ('num', num_pipeline_mm, other_numeric),\n",
    "    ('bin', 'passthrough', binary_cols)\n",
    "])\n",
    "\n",
    "# Create X,y and train/test split\n",
    "if target_col is None:\n",
    "    raise ValueError('Target column not found automatically; please set target_col to the correct column name in the notebook')\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Val shape:', X_val.shape)\n",
    "\n",
    "# Fit transformers on training data and transform a small sample to show shapes\n",
    "ct_std.fit(X_train)\n",
    "X_train_std = ct_std.transform(X_train)\n",
    "X_val_std = ct_std.transform(X_val)\n",
    "print('After Standard pipeline transform — train shape:', X_train_std.shape, 'val shape:', X_val_std.shape)\n",
    "\n",
    "ct_mm.fit(X_train)\n",
    "X_train_mm = ct_mm.transform(X_train)\n",
    "print('After MinMax pipeline transform — train shape:', X_train_mm.shape)\n",
    "\n",
    "# Save preprocessed numpy arrays for later modelling convenience\n",
    "import numpy as np\n",
    "np.save('/mnt/data/X_train_std.npy', X_train_std)\n",
    "np.save('/mnt/data/X_val_std.npy', X_val_std)\n",
    "np.save('/mnt/data/y_train.npy', y_train.to_numpy())\n",
    "np.save('/mnt/data/y_val.npy', y_val.to_numpy())\n",
    "print('\\nSaved preprocessed arrays to /mnt/data: X_train_std.npy, X_val_std.npy, y_train.npy, y_val.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e57e63",
   "metadata": {},
   "source": [
    "## 4) Next steps (Modeling)\n",
    "With the preprocessed arrays saved in `/mnt/data`, it's trivial to train models in separate notebook cells or notebooks:\n",
    "\n",
    "- Load `X_train_std.npy`, `y_train.npy` and plug into scikit-learn models (LogisticRegression, DecisionTree, RandomForest, SVM, MLPClassifier) — all covered in class.\n",
    "- When training neural networks, use `StandardScaler`-based preprocessed data (or re-fit `StandardScaler` inside a pipeline).\n",
    "- Evaluate using stratified cross-validation and metrics appropriate for multiclass (accuracy, macro-F1, confusion matrix).\n",
    "\n",
    "A good practice is to compare performance for both Standard and MinMax scaled versions to justify choice of scaler for the final models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
